"""
ä¸¤é˜¶æ®µæ æ …æ¨¡å‹ï¼ˆHurdle Modelï¼‰ç¬¬äºŒé˜¶æ®µï¼šå›å½’æ¨¡å‹ï¼ˆThe Quantifierï¼‰
=====================================================================

æœ¬è„šæœ¬å®ç°ä¸¥è°¨çš„å¤šå…ƒçº¿æ€§å›å½’åˆ†æï¼Œç”¨äºé¢„æµ‹æ½œåœ¨çƒ­é—¨è§†é¢‘çš„æ’­æ”¾é‡ã€‚

ä½œè€…ï¼šç»Ÿè®¡åˆ†æå›¢é˜Ÿ
æ—¥æœŸï¼š2024
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

# Statsmodels ç›¸å…³å¯¼å…¥
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.outliers_influence import variance_inflation_factor

# è®¾ç½®å›¾è¡¨é£æ ¼ï¼ˆä½¿ç”¨è‹±æ–‡æ ‡ç­¾ä»¥é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

# ============================================================================
# Step 0: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ============================================================================

def load_and_prepare_data(filepath):
    """
    åŠ è½½æ•°æ®å¹¶è¿›è¡ŒåŸºç¡€é¢„å¤„ç†
    
    Parameters:
    -----------
    filepath : str
        æ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
    --------
    df : DataFrame
        é¢„å¤„ç†åçš„æ•°æ®æ¡†
    """
    print("=" * 80)
    print("Step 0: Data Loading and Preprocessing")
    print("=" * 80)
    
    df = pd.read_csv(filepath)
    print(f"Original data shape: {df.shape}")
    
    # æ£€æŸ¥ç›®æ ‡å˜é‡
    if 'view_count' in df.columns:
        df['views'] = df['view_count']
    elif 'views' not in df.columns:
        raise ValueError("Data must contain 'view_count' or 'views' column")
    
    # ç­›é€‰éé›¶æ’­æ”¾é‡ï¼ˆç¬¦åˆæˆªæ–­å›å½’é€»è¾‘ï¼‰
    initial_count = len(df)
    df = df[df['views'] > 0].copy()
    print(f"Filtered data shape: {df.shape} (removed {initial_count - len(df)} zero values)")
    
    return df


# ============================================================================
# Step 1: å˜é‡å˜æ¢ä¸ç‰¹å¾å·¥ç¨‹
# ============================================================================

def feature_engineering(df):
    # æ‰§è¡Œå˜é‡å˜æ¢å’Œç‰¹å¾å·¥ç¨‹
    
    # ç»Ÿè®¡å­¦æ„ä¹‰ï¼š
    # - Logå˜æ¢ï¼šé™ä½ååº¦ï¼Œä½¿æ•°æ®æ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œæ»¡è¶³çº¿æ€§å›å½’çš„æ­£æ€æ€§å‡è®¾
    # - ä½¿ç”¨åŸå§‹äº’åŠ¨æ•°ï¼ˆå¯¹æ•°å˜æ¢åï¼‰è€Œéäº’åŠ¨ç‡ï¼Œé¿å…åå‘å› æœå…³ç³»
    #   å› ä¸º like_rate = likes / view_countï¼Œcomment_rate = comment_count / view_count
    #   äº’åŠ¨ç‡çš„åˆ†æ¯å°±æ˜¯ç›®æ ‡å˜é‡ï¼Œä¼šå¯¼è‡´è´Ÿç›¸å…³ï¼ˆåå‘å› æœå…³ç³»ï¼‰
    
    print("\n" + "=" * 80)
    print("Step 1: Variable Transformation and Feature Engineering")
    print("=" * 80)
    
    # 1.1 ç›®æ ‡å˜é‡ Log å˜æ¢
    # ä½¿ç”¨ log1p = log(1+x) ä»¥é¿å… log(0) çš„é—®é¢˜
    df['log_views'] = np.log1p(df['views'])
    print(f"\nâœ“ Target variable 'views' has been log-transformed (log1p)")
    print(f"  Original views stats: mean={df['views'].mean():.2f}, median={df['views'].median():.2f}")
    print(f"  Log-transformed stats: mean={df['log_views'].mean():.4f}, median={df['log_views'].median():.4f}")
    
    # æ³¨æ„ï¼šä¸ä½¿ç”¨ç‚¹èµå’Œè¯„è®ºç›¸å…³ç‰¹å¾ï¼Œå› ä¸ºè¿™äº›æ˜¯è§†é¢‘å‘å¸ƒåçš„æ•°æ®ï¼Œä¸èƒ½ç”¨äºé¢„æµ‹
    # åªä½¿ç”¨è§†é¢‘å‘å¸ƒå‰å¯ä»¥è·å¾—çš„å…ƒæ•°æ®ç‰¹å¾
    
    return df


# ============================================================================
# Step 2: å»ºç«‹ OLS æ¨¡å‹ (Base Model)
# ============================================================================

def build_ols_model(df):
    """
    æ„å»ºå¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹
    
    ç»Ÿè®¡å­¦æ„ä¹‰ï¼š
    - OLS (Ordinary Least Squares) æ˜¯æœ€åŸºç¡€çš„çº¿æ€§å›å½’æ–¹æ³•
    - RÂ²: æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
    - F-statistic: æ•´ä½“æ¨¡å‹æ˜¾è‘—æ€§æ£€éªŒ
    - P-values: å„ç³»æ•°çš„æ˜¾è‘—æ€§æ£€éªŒ
    
    æ³¨æ„ï¼šå¯¹äºåˆ†ç±»ç‰¹å¾ç»„ï¼ˆcategory_* å’Œ period_*ï¼‰ï¼Œæ ¹æ®æ–‡æ¡£ï¼š
    - category_24 å·²åˆ é™¤ä½œä¸ºå‚ç…§ç»„
    - period_Afternoon å·²åˆ é™¤ä½œä¸ºå‚ç…§ç»„
    è¿™æ˜¯æ ‡å‡†çš„è™šæ‹Ÿå˜é‡å¤„ç†æ–¹å¼ï¼Œé¿å…å®Œå…¨å¤šé‡å…±çº¿æ€§
    
    Parameters:
    -----------
    df : DataFrame
        ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®æ¡†
    
    Returns:
    --------
    model : RegressionResults
        æ‹Ÿåˆçš„æ¨¡å‹å¯¹è±¡
    formula : str
        ä½¿ç”¨çš„å›å½’å…¬å¼
    """
    print("\n" + "=" * 80)
    print("Step 2: Building OLS Model (Base Model)")
    print("=" * 80)
    
    # é€‰æ‹©ç‰¹å¾å˜é‡
    # æ ¹æ®æ–‡æ¡£ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸»è¦çš„ç‰¹å¾ç±»åˆ«
    feature_vars = []
    
    # åˆ†ç±»ç‰¹å¾ (category_*)
    # æ³¨æ„ï¼šcategory_24 å·²ä½œä¸ºå‚ç…§ç»„åˆ é™¤ï¼Œä¸éœ€è¦å¤„ç†
    category_cols = [col for col in df.columns if col.startswith('category_')]
    feature_vars.extend(category_cols)
    print(f"\nâœ“ Added {len(category_cols)} category features (category_24 is reference group, excluded)")
    
    # æ—¶é—´ç‰¹å¾
    # æ³¨æ„ï¼šperiod_Afternoon å·²ä½œä¸ºå‚ç…§ç»„åˆ é™¤ï¼Œä¸éœ€è¦å¤„ç†
    time_cols = ['period_Dawn', 'period_Evening', 'period_Morning', 'is_weekend']
    available_time_cols = [col for col in time_cols if col in df.columns]
    feature_vars.extend(available_time_cols)
    print(f"âœ“ Added {len(available_time_cols)} time features (period_Afternoon is reference group, excluded)")
    
    # äº’åŠ¨ä¸æ ‡é¢˜ç‰¹å¾
    # æ³¨æ„ï¼šä¸ä½¿ç”¨ log_likes å’Œ log_comment_countï¼Œå› ä¸ºè¿™äº›æ˜¯è§†é¢‘å‘å¸ƒåçš„æ•°æ®ï¼Œä¸èƒ½ç”¨äºé¢„æµ‹
    # åªä½¿ç”¨è§†é¢‘å‘å¸ƒå‰å¯ä»¥è·å¾—çš„ç‰¹å¾
    interaction_cols = ['title_length', 'title_upper_ratio', 'title_has_punct']
    available_interaction_cols = [col for col in interaction_cols if col in df.columns]
    feature_vars.extend(available_interaction_cols)
    print(f"âœ“ Added {len(available_interaction_cols)} title features")
    print("  (Note: Excluding likes/comments as they are post-publication data)")
    
    # é¢‘é“ç‰¹å¾ï¼ˆä½¿ç”¨å¯¹æ•°å˜æ¢ç‰ˆæœ¬ï¼‰
    # æ³¨æ„ï¼šlog_channel_avg_comment_count å¯èƒ½ä¹ŸåŒ…å«å‘å¸ƒåæ•°æ®ï¼Œä½†é¢‘é“å†å²å¹³å‡æ•°æ®å¯ä»¥ä½œä¸ºé¢„æµ‹ç‰¹å¾
    channel_cols = ['log_channel_activity', 'log_channel_avg_views', 
                   'log_channel_avg_comment_count', 'channel_name_len']
    available_channel_cols = [col for col in channel_cols if col in df.columns]
    feature_vars.extend(available_channel_cols)
    print(f"âœ“ Added {len(available_channel_cols)} channel features")
    
    # æ–‡æœ¬è¡ç”Ÿç‰¹å¾
    text_cols = ['log_tags_count', 'tag_density', 'log_desc_length', 
                'desc_has_timestamp', 'desc_keyword_count']
    available_text_cols = [col for col in text_cols if col in df.columns]
    feature_vars.extend(available_text_cols)
    print(f"âœ“ Added {len(available_text_cols)} text-derived features")
    
    # æ³¨æ„ï¼šlike_rate å’Œ comment_rate å·²ç»åœ¨ interaction_cols ä¸­ï¼Œä¸éœ€è¦é¢å¤–æ·»åŠ 
    
    # ç§»é™¤ä¸åœ¨æ•°æ®ä¸­çš„å˜é‡
    feature_vars = [var for var in feature_vars if var in df.columns]
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing = df[feature_vars + ['log_views']].isnull().sum()
    if missing.sum() > 0:
        print(f"\nâš  Found missing values, will remove rows with missing values:")
        print(missing[missing > 0])
        df = df.dropna(subset=feature_vars + ['log_views'])
        print(f"  Data shape after removal: {df.shape}")
    
    # æ„å»ºå›å½’å…¬å¼
    # æ ¼å¼: "log_views ~ var1 + var2 + ..."
    formula = "log_views ~ " + " + ".join(feature_vars)
    print(f"\nRegression formula contains {len(feature_vars)} feature variables")
    print(f"Formula preview: log_views ~ ... + log_likes + log_comment_count")
    
    # æ‹Ÿåˆ OLS æ¨¡å‹
    print("\nFitting OLS model...")
    model = ols(formula, data=df).fit()
    
    # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 80)
    print("OLS Model Statistical Summary")
    print("=" * 80)
    print(model.summary())
    
    # æå–å…³é”®ç»Ÿè®¡é‡
    print("\n" + "-" * 80)
    print("Key Statistics Summary:")
    print("-" * 80)
    print(f"R-squared (RÂ²):                    {model.rsquared:.4f}")
    print(f"Adjusted R-squared:                 {model.rsquared_adj:.4f}")
    print(f"F-statistic:                       {model.fvalue:.4f}")
    print(f"F-statistic p-value:               {model.f_pvalue:.2e}")
    print(f"Model significance:                 {'Significant' if model.f_pvalue < 0.05 else 'Not significant'} (Î±=0.05)")
    
    # æ˜¾ç¤ºå…³é”®æ˜¾è‘—å˜é‡çš„ç³»æ•°
    print(f"\nKey Significant Variables (p < 0.05):")
    significant_vars = model.params[model.pvalues < 0.05].index.tolist()
    significant_vars = [v for v in significant_vars if v != 'Intercept']
    for var in significant_vars[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
        coef = model.params[var]
        pval = model.pvalues[var]
        print(f"  {var}: {coef:.6f} (p={pval:.4f})")
    
    # ä¿å­˜æ¨¡å‹ç³»æ•°åˆ°æ–‡ä»¶ï¼ˆç”¨äºåŸå‹ç³»ç»Ÿï¼‰
    save_model_coefficients(model, formula)
    
    return model, formula, df


# ============================================================================
# ä¿å­˜æ¨¡å‹ç³»æ•°ï¼ˆç”¨äºåŸå‹ç³»ç»Ÿï¼‰
# ============================================================================

def save_model_coefficients(model, formula):
    """
    ä¿å­˜æ¨¡å‹ç³»æ•°åˆ°æ–‡ä»¶ï¼Œä¾›åŸå‹ç³»ç»Ÿä½¿ç”¨
    
    Parameters:
    -----------
    model : RegressionResults
        æ‹Ÿåˆçš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ç¨³å¥æ ‡å‡†è¯¯æ¨¡å‹ï¼‰
    formula : str
        å›å½’å…¬å¼
    """
    import json
    
    # å®‰å…¨åœ°è·å–å‚æ•°åç§°å’Œå€¼
    # å¯¹äºç¨³å¥æ ‡å‡†è¯¯æ¨¡å‹ï¼Œparams å¯èƒ½æ˜¯æ•°ç»„è€Œä¸æ˜¯ Series
    try:
        # å°è¯•è·å–å‚æ•°åç§°
        if hasattr(model.params, 'index'):
            param_names = list(model.params.index)
            param_values = model.params.values if hasattr(model.params, 'values') else np.array(model.params)
            bse_values = model.bse.values if hasattr(model.bse, 'values') else np.array(model.bse)
            pval_values = model.pvalues.values if hasattr(model.pvalues, 'values') else np.array(model.pvalues)
        else:
            # å¦‚æœæ˜¯æ•°ç»„ï¼Œä»åŸå§‹æ¨¡å‹è·å–åç§°
            param_names = list(model.model.exog_names)
            param_values = np.array(model.params)
            bse_values = np.array(model.bse)
            pval_values = np.array(model.pvalues)
    except:
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä»åŸå§‹æ¨¡å‹è·å–
        param_names = list(model.model.exog_names)
        param_values = np.array(model.params)
        bse_values = np.array(model.bse)
        pval_values = np.array(model.pvalues)
    
    # æ‰¾åˆ°æˆªè·çš„ç´¢å¼•
    intercept_idx = None
    for i, name in enumerate(param_names):
        if name == 'Intercept' or name == 'const':
            intercept_idx = i
            break
    
    if intercept_idx is None:
        intercept_idx = 0  # é»˜è®¤ç¬¬ä¸€ä¸ªæ˜¯æˆªè·
    
    # åˆ›å»ºç³»æ•°å­—å…¸
    coefficients_dict = {
        'intercept': float(param_values[intercept_idx]),
        'coefficients': {},
        'formula': formula,
        'model_info': {
            'r_squared': float(model.rsquared),
            'adj_r_squared': float(model.rsquared_adj),
            'f_statistic': float(model.fvalue),
            'f_pvalue': float(model.f_pvalue),
            'n_observations': int(model.nobs),
            'n_features': len(param_names) - 1
        }
    }
    
    # ä¿å­˜æ‰€æœ‰ç³»æ•°ï¼ˆæ’é™¤æˆªè·ï¼‰
    for i, var in enumerate(param_names):
        if i != intercept_idx and var not in ['Intercept', 'const']:
            coefficients_dict['coefficients'][var] = {
                'coefficient': float(param_values[i]),
                'std_err': float(bse_values[i]),
                'p_value': float(pval_values[i]),
                'is_significant': bool(pval_values[i] < 0.05)
            }
    
    # ä¿å­˜ä¸º JSON æ–‡ä»¶
    with open('model_coefficients.json', 'w', encoding='utf-8') as f:
        json.dump(coefficients_dict, f, indent=2, ensure_ascii=False)
    
    # åŒæ—¶ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    coeff_df = pd.DataFrame({
        'variable': param_names,
        'coefficient': param_values,
        'std_err': bse_values,
        'p_value': pval_values,
        'is_significant': pval_values < 0.05
    })
    coeff_df.to_csv('model_coefficients.csv', index=False, encoding='utf-8-sig')
    
    print(f"\nâœ“ Model coefficients saved to:")
    print(f"  - model_coefficients.json (for prototype system)")
    print(f"  - model_coefficients.csv (for reference)")
    
    return coefficients_dict


# ============================================================================
# Step 3: æ¨¡å‹è¯Šæ–­ (Diagnostics)
# ============================================================================

def model_diagnostics(model, df):
    """
    æ‰§è¡Œå…¨é¢çš„æ¨¡å‹è¯Šæ–­
    
    åŒ…æ‹¬ï¼š
    1. æ®‹å·®åˆ†æï¼ˆæ®‹å·® vs æ‹Ÿåˆå€¼å›¾ã€Q-Qå›¾ï¼‰
    2. å¼‚æ–¹å·®æ£€éªŒï¼ˆBreusch-Pagan Testï¼‰
    3. å¦‚æœå­˜åœ¨å¼‚æ–¹å·®ï¼Œä½¿ç”¨ç¨³å¥æ ‡å‡†è¯¯é‡æ–°æ‹Ÿåˆ
    
    Parameters:
    -----------
    model : RegressionResults
        åˆå§‹ OLS æ¨¡å‹
    df : DataFrame
        æ•°æ®æ¡†
    
    Returns:
    --------
    robust_model : RegressionResults or None
        å¦‚æœå­˜åœ¨å¼‚æ–¹å·®ï¼Œè¿”å›ä½¿ç”¨ç¨³å¥æ ‡å‡†è¯¯çš„æ¨¡å‹ï¼›å¦åˆ™è¿”å› None
    """
    print("\n" + "=" * 80)
    print("Step 3: Model Diagnostics")
    print("=" * 80)
    
    # è·å–æ®‹å·®å’Œæ‹Ÿåˆå€¼
    fitted_values = model.fittedvalues
    residuals = model.resid
    standardized_residuals = residuals / np.sqrt(model.mse_resid)
    
    # 3.1 æ®‹å·®åˆ†æå¯è§†åŒ–
    print("\n3.1 Residual Analysis Visualization")
    print("-" * 80)
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Model Diagnostics', fontsize=16, fontweight='bold')
    
    # 3.1.1 æ®‹å·® vs æ‹Ÿåˆå€¼å›¾
    ax1 = axes[0, 0]
    ax1.scatter(fitted_values, residuals, alpha=0.5, s=20, edgecolors='k', linewidth=0.5)
    ax1.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero residual line')
    ax1.set_xlabel('Fitted Values', fontsize=12)
    ax1.set_ylabel('Residuals', fontsize=12)
    ax1.set_title('Residuals vs Fitted', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # æ·»åŠ å¹³æ»‘æ›²çº¿ï¼ˆä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰
    try:
        # å¯¹æ•°æ®è¿›è¡Œæ’åº
        sorted_idx = np.argsort(fitted_values)
        sorted_fitted = fitted_values[sorted_idx]
        sorted_residuals = residuals[sorted_idx]
        
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡åˆ›å»ºå¹³æ»‘æ›²çº¿
        window_size = max(50, len(fitted_values) // 20)  # çª—å£å¤§å°ä¸ºæ•°æ®ç‚¹çš„5%
        if window_size % 2 == 0:
            window_size += 1  # ç¡®ä¿æ˜¯å¥‡æ•°
        
        from scipy.signal import savgol_filter
        if len(sorted_residuals) > window_size:
            smooth_residuals = savgol_filter(sorted_residuals, window_size, 3)
            ax1.plot(sorted_fitted, smooth_residuals, 
                    'g-', linewidth=2, label='Smooth curve', alpha=0.7)
            ax1.legend()
    except Exception as e:
        # å¦‚æœå¹³æ»‘å¤±è´¥ï¼Œå°è¯•ç®€å•çš„ç§»åŠ¨å¹³å‡
        try:
            sorted_idx = np.argsort(fitted_values)
            sorted_fitted = fitted_values[sorted_idx]
            sorted_residuals = residuals[sorted_idx]
            
            # ç®€å•çš„ç§»åŠ¨å¹³å‡
            window = max(10, len(fitted_values) // 50)
            if len(sorted_residuals) > window * 2:
                smooth = np.convolve(sorted_residuals, np.ones(window)/window, mode='valid')
                smooth_fitted = sorted_fitted[window//2:len(sorted_fitted)-window//2+1]
                if len(smooth) == len(smooth_fitted):
                    ax1.plot(smooth_fitted, smooth, 
                            'g-', linewidth=2, label='Smooth curve', alpha=0.7)
                    ax1.legend()
        except:
            pass  # å¦‚æœéƒ½å¤±è´¥ï¼Œå°±ä¸æ˜¾ç¤ºå¹³æ»‘æ›²çº¿
    
    # 3.1.2 Q-Q å›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰
    ax2 = axes[0, 1]
    stats.probplot(residuals, dist="norm", plot=ax2)
    ax2.set_title('Q-Q Plot (Normality Test)', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # 3.1.3 æ ‡å‡†åŒ–æ®‹å·® vs æ‹Ÿåˆå€¼
    ax3 = axes[1, 0]
    ax3.scatter(fitted_values, standardized_residuals, alpha=0.5, s=20, edgecolors='k', linewidth=0.5)
    ax3.axhline(y=0, color='r', linestyle='--', linewidth=2)
    ax3.axhline(y=2, color='orange', linestyle='--', linewidth=1, label='Â±2Ïƒ')
    ax3.axhline(y=-2, color='orange', linestyle='--', linewidth=1)
    ax3.set_xlabel('Fitted Values', fontsize=12)
    ax3.set_ylabel('Standardized Residuals', fontsize=12)
    ax3.set_title('Standardized Residuals vs Fitted', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # 3.1.4 æ®‹å·®ç›´æ–¹å›¾
    ax4 = axes[1, 1]
    ax4.hist(residuals, bins=50, density=True, alpha=0.7, edgecolor='black', color='steelblue')
    # å åŠ æ­£æ€åˆ†å¸ƒæ›²çº¿
    mu, sigma = residuals.mean(), residuals.std()
    x = np.linspace(residuals.min(), residuals.max(), 100)
    ax4.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, 
            label=f'Normal dist (Î¼={mu:.3f}, Ïƒ={sigma:.3f})')
    ax4.set_xlabel('Residuals', fontsize=12)
    ax4.set_ylabel('Density', fontsize=12)
    ax4.set_title('Residual Distribution Histogram', fontsize=13, fontweight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_diagnostics.png', dpi=300, bbox_inches='tight')
    print("âœ“ Diagnostic plots saved as 'model_diagnostics.png'")
    plt.show()
    
    # 3.1.5 é¢å¤–çš„å¯è§†åŒ–ï¼šç³»æ•°é‡è¦æ€§å›¾å’Œå®é™…å€¼ vs é¢„æµ‹å€¼
    visualize_coefficient_importance(model)
    visualize_actual_vs_predicted(model, df)
    
    # 3.2 æ­£æ€æ€§æ£€éªŒï¼ˆShapiro-Wilk Testï¼Œé€‚ç”¨äºå°æ ·æœ¬ï¼‰
    print("\n3.2 Residual Normality Test")
    print("-" * 80)
    if len(residuals) <= 5000:
        shapiro_stat, shapiro_p = stats.shapiro(residuals)
        print(f"Shapiro-Wilk Test:")
        print(f"  Statistic: {shapiro_stat:.4f}")
        print(f"  p-value: {shapiro_p:.4f}")
        print(f"  Conclusion: {'Residuals approximately normal' if shapiro_p > 0.05 else 'Residuals significantly deviate from normal'} (Î±=0.05)")
    else:
        print("Sample size too large, skipping Shapiro-Wilk test (only for nâ‰¤5000)")
        # ä½¿ç”¨ Kolmogorov-Smirnov æ£€éªŒ
        ks_stat, ks_p = stats.kstest(residuals, 'norm', args=(mu, sigma))
        print(f"Kolmogorov-Smirnov Test:")
        print(f"  Statistic: {ks_stat:.4f}")
        print(f"  p-value: {ks_p:.4f}")
        print(f"  Conclusion: {'Residuals approximately normal' if ks_p > 0.05 else 'Residuals significantly deviate from normal'} (Î±=0.05)")
    
    # 3.3 å¼‚æ–¹å·®æ£€éªŒï¼ˆBreusch-Pagan Testï¼‰
    print("\n3.3 Heteroscedasticity Test (Breusch-Pagan Test)")
    print("-" * 80)
    
    # æ‰§è¡Œ Breusch-Pagan æ£€éªŒ
    bp_lm, bp_lm_pvalue, bp_fvalue, bp_f_pvalue = het_breuschpagan(
        residuals, model.model.exog
    )
    
    print(f"Breusch-Pagan LM statistic: {bp_lm:.4f}")
    print(f"Breusch-Pagan LM p-value: {bp_lm_pvalue:.4f}")
    print(f"Breusch-Pagan F statistic: {bp_fvalue:.4f}")
    print(f"Breusch-Pagan F p-value: {bp_f_pvalue:.4f}")
    
    if bp_lm_pvalue < 0.05:
        print(f"\nâš  Heteroscedasticity detected (p-value = {bp_lm_pvalue:.4f} < 0.05)")
        print("  Will refit model with robust standard errors (HC3)...")
        
        # ä½¿ç”¨ç¨³å¥æ ‡å‡†è¯¯é‡æ–°æ‹Ÿåˆ
        robust_model = model.get_robustcov_results(cov_type='HC3')
        
        print("\n" + "=" * 80)
        print("Model Summary with Robust Standard Errors (HC3)")
        print("=" * 80)
        print(robust_model.summary())
        
        print("\n" + "-" * 80)
        print("Key Statistics Comparison:")
        print("-" * 80)
        print(f"{'Metric':<30} {'Original Model':<20} {'Robust SE Model':<20}")
        print("-" * 70)
        print(f"{'R-squared':<30} {model.rsquared:<20.4f} {robust_model.rsquared:<20.4f}")
        print(f"{'F-statistic':<30} {model.fvalue:<20.4f} {robust_model.fvalue:<20.4f}")
        
        # æ¯”è¾ƒå…³é”®æ˜¾è‘—å˜é‡çš„æ ‡å‡†è¯¯ï¼ˆæ˜¾ç¤ºå‰5ä¸ªæœ€æ˜¾è‘—çš„ï¼‰
        significant_vars = model.params[model.pvalues < 0.05].index.tolist()
        significant_vars = [v for v in significant_vars if v != 'Intercept']
        significant_vars = sorted(significant_vars, key=lambda x: model.pvalues[x])[:5]  # å‰5ä¸ªæœ€æ˜¾è‘—çš„
        
        if significant_vars:
            print(f"\nTop 5 Significant Variables Coefficient Comparison:")
            for var in significant_vars:
                print(f"\n{var}:")
                print(f"  Original model coefficient: {model.params[var]:.6f}")
                print(f"  Original model SE: {model.bse[var]:.6f}")
                try:
                    robust_se = robust_model.bse.loc[var] if hasattr(robust_model.bse, 'loc') else robust_model.bse[var]
                    robust_pval = robust_model.pvalues.loc[var] if hasattr(robust_model.pvalues, 'loc') else robust_model.pvalues[var]
                except:
                    param_names = list(model.params.index)
                    if var in param_names:
                        idx = param_names.index(var)
                        robust_se = robust_model.bse[idx]
                        robust_pval = robust_model.pvalues[idx]
                    else:
                        robust_se = None
                        robust_pval = None
                
                if robust_se is not None:
                    print(f"  Robust SE: {robust_se:.6f}")
                    print(f"  Original model p-value: {model.pvalues[var]:.4f}")
                    print(f"  Robust model p-value: {robust_pval:.4f}")
        
        return robust_model
    else:
        print(f"\nâœ“ No significant heteroscedasticity detected (p-value = {bp_lm_pvalue:.4f} â‰¥ 0.05)")
        print("  Model satisfies homoscedasticity assumption, no need for robust standard errors")
        return None


# ============================================================================
# ç³»æ•°é‡è¦æ€§å¯è§†åŒ–
# ============================================================================

def visualize_coefficient_importance(model):
    """
    å¯è§†åŒ–æ¨¡å‹ç³»æ•°çš„é‡è¦æ€§
    
    Parameters:
    -----------
    model : RegressionResults
        æ‹Ÿåˆçš„æ¨¡å‹
    """
    print("\n3.1.5 Coefficient Importance Visualization")
    print("-" * 80)
    
    # è·å–ç³»æ•°ï¼ˆæ’é™¤æˆªè·ï¼‰
    coef_data = []
    for var in model.params.index:
        if var != 'Intercept':
            coef_data.append({
                'variable': var,
                'coefficient': model.params[var],
                'abs_coefficient': abs(model.params[var]),
                'p_value': model.pvalues[var],
                'is_significant': model.pvalues[var] < 0.05
            })
    
    coef_df = pd.DataFrame(coef_data)
    coef_df = coef_df.sort_values('abs_coefficient', ascending=False)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 1, figsize=(14, 12))
    
    # ä¸Šå›¾ï¼šç³»æ•°å¤§å°ï¼ˆæŒ‰ç»å¯¹å€¼æ’åºï¼Œæ˜¾ç¤ºå‰20ä¸ªï¼‰
    ax1 = axes[0]
    top_coefs = coef_df.head(20)
    colors = ['red' if not sig else 'steelblue' for sig in top_coefs['is_significant']]
    bars = ax1.barh(range(len(top_coefs)), top_coefs['coefficient'], color=colors, alpha=0.7)
    ax1.set_yticks(range(len(top_coefs)))
    ax1.set_yticklabels(top_coefs['variable'], fontsize=9)
    ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)
    ax1.set_xlabel('Coefficient Value', fontsize=12)
    ax1.set_title('Top 20 Variable Coefficients (Sorted by Absolute Value)', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')
    ax1.legend([plt.Rectangle((0,0),1,1, facecolor='steelblue', alpha=0.7), 
                plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7)],
               ['Significant (p<0.05)', 'Not Significant'], loc='best')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (idx, row) in enumerate(top_coefs.iterrows()):
        ax1.text(row['coefficient'], i, f"  {row['coefficient']:.4f}", 
                va='center', fontsize=8)
    
    # ä¸‹å›¾ï¼šç³»æ•° vs På€¼æ•£ç‚¹å›¾
    ax2 = axes[1]
    scatter = ax2.scatter(coef_df['coefficient'], -np.log10(coef_df['p_value'] + 1e-10),
                         c=coef_df['is_significant'], cmap='RdYlGn', s=100, alpha=0.6, edgecolors='black')
    ax2.axhline(y=-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='p=0.05 threshold')
    ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)
    ax2.set_xlabel('Coefficient Value', fontsize=12)
    ax2.set_ylabel('-log10(p-value)', fontsize=12)
    ax2.set_title('Coefficient vs Significance (Volcano Plot)', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best')
    
    # æ ‡æ³¨æœ€æ˜¾è‘—çš„å˜é‡
    top_sig = coef_df.nsmallest(5, 'p_value')
    for _, row in top_sig.iterrows():
        ax2.annotate(row['variable'], 
                    (row['coefficient'], -np.log10(row['p_value'] + 1e-10)),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.tight_layout()
    plt.savefig('coefficient_importance.png', dpi=300, bbox_inches='tight')
    print("âœ“ Coefficient importance plots saved as 'coefficient_importance.png'")
    plt.show()


# ============================================================================
# å®é™…å€¼ vs é¢„æµ‹å€¼å¯è§†åŒ–
# ============================================================================

def visualize_actual_vs_predicted(model, df):
    """
    å¯è§†åŒ–å®é™…å€¼ vs é¢„æµ‹å€¼
    
    Parameters:
    -----------
    model : RegressionResults
        æ‹Ÿåˆçš„æ¨¡å‹
    df : DataFrame
        æ•°æ®æ¡†
    """
    print("\n3.1.6 Actual vs Predicted Values Visualization")
    print("-" * 80)
    
    # è·å–å®é™…å€¼å’Œé¢„æµ‹å€¼
    actual_log = df['log_views'].values
    predicted_log = model.fittedvalues.values
    
    # è½¬æ¢å›åŸå§‹å°ºåº¦
    actual_views = np.expm1(actual_log)
    predicted_views = np.expm1(predicted_log)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # å·¦å›¾ï¼šå¯¹æ•°å°ºåº¦
    ax1 = axes[0]
    ax1.scatter(actual_log, predicted_log, alpha=0.5, s=10, edgecolors='k', linewidth=0.3)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿ï¼ˆy=xï¼‰
    min_val = min(actual_log.min(), predicted_log.min())
    max_val = max(actual_log.max(), predicted_log.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction (y=x)')
    
    # è®¡ç®— RÂ²
    r2 = r2_score(actual_log, predicted_log)
    ax1.text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=ax1.transAxes, 
            fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    ax1.set_xlabel('Actual log(views)', fontsize=12)
    ax1.set_ylabel('Predicted log(views)', fontsize=12)
    ax1.set_title('Actual vs Predicted (Log Scale)', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='best')
    
    # å³å›¾ï¼šåŸå§‹å°ºåº¦
    ax2 = axes[1]
    # ä¸ºäº†å¯è§†åŒ–æ•ˆæœï¼Œé™åˆ¶èŒƒå›´ï¼ˆé¿å…æç«¯å€¼ï¼‰
    max_display = np.percentile(actual_views, 95)
    mask = (actual_views <= max_display) & (predicted_views <= max_display)
    
    ax2.scatter(actual_views[mask], predicted_views[mask], alpha=0.5, s=10, edgecolors='k', linewidth=0.3)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
    min_val = min(actual_views[mask].min(), predicted_views[mask].min())
    max_val = max(actual_views[mask].max(), predicted_views[mask].max())
    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction (y=x)')
    
    # è®¡ç®— RÂ²ï¼ˆåŸå§‹å°ºåº¦ï¼‰
    r2_orig = r2_score(actual_views, predicted_views)
    ax2.text(0.05, 0.95, f'RÂ² = {r2_orig:.4f}', transform=ax2.transAxes, 
            fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax2.text(0.05, 0.88, f'Displaying 95% of data', transform=ax2.transAxes, 
            fontsize=10, verticalalignment='top', style='italic')
    
    ax2.set_xlabel('Actual Views', fontsize=12)
    ax2.set_ylabel('Predicted Views', fontsize=12)
    ax2.set_title('Actual vs Predicted (Original Scale)', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='best')
    
    plt.tight_layout()
    plt.savefig('actual_vs_predicted.png', dpi=300, bbox_inches='tight')
    print("âœ“ Actual vs predicted plots saved as 'actual_vs_predicted.png'")
    plt.show()


# ============================================================================
# Step 4: é¢„æµ‹ä¸åŒºé—´ä¼°è®¡ (Inference)
# ============================================================================

def prediction_with_intervals(model, df, scenario='typical'):
    """
    å¯¹æ–°æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è®¡ç®—é¢„æµ‹åŒºé—´
    
    ç»Ÿè®¡å­¦æ„ä¹‰ï¼š
    - ç‚¹é¢„æµ‹ï¼šåŸºäºæ¨¡å‹ç³»æ•°çš„æœŸæœ›å€¼é¢„æµ‹
    - é¢„æµ‹åŒºé—´ï¼šè€ƒè™‘æ¨¡å‹ä¸ç¡®å®šæ€§å’Œéšæœºè¯¯å·®çš„åŒºé—´ä¼°è®¡
    - 95% é¢„æµ‹åŒºé—´æ„å‘³ç€æœ‰ 95% çš„æ¦‚ç‡çœŸå®å€¼è½åœ¨æ­¤åŒºé—´å†…
    
    Parameters:
    -----------
    model : RegressionResults
        æ‹Ÿåˆçš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ç¨³å¥æ ‡å‡†è¯¯æ¨¡å‹ï¼‰
    df : DataFrame
        è®­ç»ƒæ•°æ®æ¡†
    scenario : str
        é¢„æµ‹åœºæ™¯ï¼š'typical' (å…¸å‹), 'high_potential' (é«˜æ½œåŠ›), 'low_potential' (ä½æ½œåŠ›)
    
    Returns:
    --------
    prediction_dict : dict
        åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
    """
    print("\n" + "=" * 80)
    print("Step 4: Prediction and Interval Estimation (Inference)")
    print("=" * 80)
    
    # åˆ›å»ºæ–°æ ·æœ¬çš„ç‰¹å¾å€¼
    print(f"\nCreating simulated new sample (Scenario: {scenario}):")
    
    # æ ¹æ®åœºæ™¯è®¾ç½®ç‰¹å¾å€¼
    if scenario == 'high_potential':
        # é«˜æ½œåŠ›åœºæ™¯ï¼šä½¿ç”¨ä¸Šå››åˆ†ä½æ•°ï¼ˆå¯¹äºæ­£å‘ç‰¹å¾ï¼‰æˆ–ä¸‹å››åˆ†ä½æ•°ï¼ˆå¯¹äºè´Ÿå‘ç‰¹å¾ï¼‰
        print("  Using high potential values (75th percentile for positive features)")
    elif scenario == 'low_potential':
        # ä½æ½œåŠ›åœºæ™¯ï¼šä½¿ç”¨ä¸‹å››åˆ†ä½æ•°ï¼ˆå¯¹äºæ­£å‘ç‰¹å¾ï¼‰æˆ–ä¸Šå››åˆ†ä½æ•°ï¼ˆå¯¹äºè´Ÿå‘ç‰¹å¾ï¼‰
        print("  Using low potential values (25th percentile for positive features)")
    else:
        # å…¸å‹åœºæ™¯ï¼šä½¿ç”¨ä¸­ä½æ•°
        print("  Using typical values (median)")
    
    # åˆ›å»ºæ–°æ ·æœ¬çš„ DataFrame
    # æ³¨æ„ï¼šä½¿ç”¨ formula API æ—¶ï¼Œå¿…é¡»ä¼ é€’åŒ…å«æ‰€æœ‰å˜é‡çš„ DataFrame
    # æœ€å¯é çš„æ–¹æ³•ï¼šä»åŸå§‹è®­ç»ƒæ•°æ®å¤åˆ¶ä¸€è¡Œï¼Œç„¶åä¿®æ”¹éœ€è¦çš„å€¼
    # è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰å˜é‡éƒ½å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
    
    # ä»åŸå§‹æ•°æ®ä¸­å–ç¬¬ä¸€è¡Œä½œä¸ºæ¨¡æ¿ï¼ˆæ’é™¤ç›®æ ‡å˜é‡ï¼‰
    # ç„¶åä¿®æ”¹æˆ‘ä»¬éœ€è¦çš„å€¼
    new_sample = df.iloc[[0]].copy()  # ä½¿ç”¨ [[0]] ä¿æŒ DataFrame æ ¼å¼
    
    # ç§»é™¤ç›®æ ‡å˜é‡åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'log_views' in new_sample.columns:
        new_sample = new_sample.drop(columns=['log_views'])
    if 'views' in new_sample.columns:
        new_sample = new_sample.drop(columns=['views'])
    if 'view_count' in new_sample.columns:
        new_sample = new_sample.drop(columns=['view_count'])
    
    # å®‰å…¨åœ°è·å–æ¨¡å‹å‚æ•°åç§°åˆ—è¡¨
    # å¤„ç†ç¨³å¥æ ‡å‡†è¯¯æ¨¡å‹ï¼ˆparams å¯èƒ½æ˜¯æ•°ç»„ï¼‰
    try:
        if hasattr(model.params, 'index'):
            model_var_names = list(model.params.index)
            # åˆ›å»ºå‚æ•°ååˆ°ç³»æ•°çš„æ˜ å°„
            param_dict = {name: model.params[name] for name in model_var_names}
        else:
            # å¦‚æœæ˜¯æ•°ç»„ï¼Œä»åŸå§‹æ¨¡å‹è·å–åç§°
            model_var_names = list(model.model.exog_names)
            param_values = np.array(model.params)
            param_dict = {name: param_values[i] for i, name in enumerate(model_var_names)}
    except:
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        model_var_names = list(model.model.exog_names)
        param_values = np.array(model.params)
        param_dict = {name: param_values[i] for i, name in enumerate(model_var_names)}
    
    # è®¾ç½®æ‰€æœ‰åˆ†ç±»å˜é‡ä¸º0ï¼ˆå‚ç…§ç»„ï¼‰
    # åŒæ—¶å¤„ç†å­—ç¬¦ä¸²åˆ—å’Œæ•°å€¼åˆ—
    for col in new_sample.columns:
        if col.startswith('category_'):
            new_sample[col] = 0
        elif col.startswith('period_'):
            new_sample[col] = 0
        elif col == 'is_weekend':
            new_sample[col] = 0
        elif col == 'title_has_punct':
            new_sample[col] = 0
        elif col == 'desc_has_timestamp':
            new_sample[col] = 0
        elif col == 'desc_has_youtube_link':
            new_sample[col] = 0
        elif col == 'channel_has_digit':
            new_sample[col] = 0
        elif col == 'channel_has_special':
            new_sample[col] = 0
        elif col in ['title', 'publishedAt', 'trending_date', 'tags', 'categoryId']:
            # å­—ç¬¦ä¸²åˆ—æˆ–IDåˆ—ï¼šä¿æŒåŸå€¼ï¼ˆè¿™äº›åˆ—ä¸åœ¨æ¨¡å‹ä¸­ï¼Œä½†éœ€è¦å­˜åœ¨ä»¥é¿å…é”™è¯¯ï¼‰
            # å®é™…ä¸Šï¼Œå¦‚æœè¿™äº›åˆ—ä¸åœ¨å…¬å¼ä¸­ï¼Œpatsy ä¼šå¿½ç•¥å®ƒä»¬
            pass  # ä¿æŒåŸå€¼
        else:
            # å…¶ä»–è¿ç»­å˜é‡ï¼šæ ¹æ®åœºæ™¯è®¾ç½®å€¼
            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
                # æ£€æŸ¥å˜é‡æ˜¯å¦åœ¨æ¨¡å‹ä¸­
                if col in param_dict:
                    coef = param_dict[col]
                    # æ ¹æ®åœºæ™¯å’Œç³»æ•°æ–¹å‘è®¾ç½®å€¼
                    if scenario == 'high_potential':
                        # é«˜æ½œåŠ›ï¼šæ­£å‘ç³»æ•°ç”¨ä¸Šå››åˆ†ä½æ•°ï¼Œè´Ÿå‘ç³»æ•°ç”¨ä¸‹å››åˆ†ä½æ•°
                        if coef > 0:
                            new_sample[col] = df[col].quantile(0.75)
                        else:
                            new_sample[col] = df[col].quantile(0.25)
                    elif scenario == 'low_potential':
                        # ä½æ½œåŠ›ï¼šæ­£å‘ç³»æ•°ç”¨ä¸‹å››åˆ†ä½æ•°ï¼Œè´Ÿå‘ç³»æ•°ç”¨ä¸Šå››åˆ†ä½æ•°
                        if coef > 0:
                            new_sample[col] = df[col].quantile(0.25)
                        else:
                            new_sample[col] = df[col].quantile(0.75)
                    else:
                        # å…¸å‹åœºæ™¯ï¼šä½¿ç”¨ä¸­ä½æ•°
                        new_sample[col] = df[col].median()
                else:
                    # å˜é‡ä¸åœ¨æ¨¡å‹ä¸­ï¼Œä½¿ç”¨ä¸­ä½æ•°
                    new_sample[col] = df[col].median()
                # å¦‚æœæ˜¯éæ•°å€¼åˆ—ï¼ˆå­—ç¬¦ä¸²ã€æ—¥æœŸç­‰ï¼‰ï¼Œä¿æŒåŸå€¼
    
    # ä½¿ç”¨ DataFrame è¿›è¡Œé¢„æµ‹ï¼ˆformula API è¦æ±‚ï¼‰
    # formula API ä¼šè‡ªåŠ¨ä» DataFrame ä¸­æå–å…¬å¼éœ€è¦çš„å˜é‡
    X_new = new_sample
    
    # ç‚¹é¢„æµ‹ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    log_pred = model.predict(X_new)[0]
    
    # é¢„æµ‹åŒºé—´ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    # ä½¿ç”¨ get_prediction æ–¹æ³•è·å–é¢„æµ‹åŒºé—´
    pred_result = model.get_prediction(X_new)
    pred_ci = pred_result.conf_int(alpha=0.05)  # 95% ç½®ä¿¡åŒºé—´
    
    # æ³¨æ„ï¼šè¿™é‡Œå¾—åˆ°çš„æ˜¯ç½®ä¿¡åŒºé—´ï¼Œä¸æ˜¯é¢„æµ‹åŒºé—´
    # é¢„æµ‹åŒºé—´éœ€è¦è€ƒè™‘æ®‹å·®çš„æ ‡å‡†è¯¯
    # è®¡ç®—é¢„æµ‹åŒºé—´çš„æ ‡å‡†è¯¯
    mse = model.mse_resid
    pred_se = np.sqrt(mse + pred_result.var_pred_mean[0])
    
    # 95% é¢„æµ‹åŒºé—´ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    t_critical = stats.t.ppf(0.975, model.df_resid)
    log_pred_lower = log_pred - t_critical * pred_se
    log_pred_upper = log_pred + t_critical * pred_se
    
    # è½¬æ¢å›åŸå§‹å°ºåº¦ï¼ˆæ’­æ”¾é‡ï¼‰
    pred_views = np.expm1(log_pred)
    pred_views_lower = np.expm1(log_pred_lower)
    pred_views_upper = np.expm1(log_pred_upper)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "-" * 80)
    print("Prediction Results:")
    print("-" * 80)
    print(f"Log scale (log_views):")
    print(f"  Point prediction: {log_pred:.4f}")
    print(f"  95% prediction interval: [{log_pred_lower:.4f}, {log_pred_upper:.4f}]")
    print(f"\nOriginal scale (views):")
    print(f"  Point prediction: {pred_views:,.0f} views")
    print(f"  95% prediction interval: [{pred_views_lower:,.0f}, {pred_views_upper:,.0f}] views")
    print(f"  Interval width: {pred_views_upper - pred_views_lower:,.0f} views")
    
    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # ç»˜åˆ¶é¢„æµ‹åŒºé—´
    ax.barh([0], [pred_views_upper - pred_views_lower], 
            left=pred_views_lower, height=0.3, 
            alpha=0.3, color='steelblue', label='95% Prediction Interval')
    ax.plot([pred_views], [0], 'ro', markersize=12, label='Point Prediction', zorder=5)
    ax.errorbar([pred_views], [0], 
                xerr=[[pred_views - pred_views_lower], [pred_views_upper - pred_views]], 
                fmt='none', ecolor='red', elinewidth=2, capsize=10, capthick=2, zorder=4)
    
    ax.set_xlabel('Predicted Views', fontsize=12)
    ax.set_yticks([0])
    ax.set_yticklabels([scenario.replace('_', ' ').title()])
    title_text = f'Views Prediction Result\n(Scenario: {scenario.replace("_", " ").title()})'
    ax.set_title(title_text, fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    ax.legend(loc='best')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    ax.text(pred_views, 0.15, f'{pred_views:,.0f}', 
           ha='center', va='bottom', fontsize=11, fontweight='bold')
    ax.text(pred_views_lower, -0.2, f'{pred_views_lower:,.0f}', 
           ha='center', va='top', fontsize=9, color='blue')
    ax.text(pred_views_upper, -0.2, f'{pred_views_upper:,.0f}', 
           ha='center', va='top', fontsize=9, color='blue')
    
    plt.tight_layout()
    plt.savefig('prediction_result.png', dpi=300, bbox_inches='tight')
    print("\nâœ“ Prediction result plot saved as 'prediction_result.png'")
    plt.show()
    
    return {
        'log_pred': log_pred,
        'log_pred_lower': log_pred_lower,
        'log_pred_upper': log_pred_upper,
        'pred_views': pred_views,
        'pred_views_lower': pred_views_lower,
        'pred_views_upper': pred_views_upper,
        'scenario': scenario
    }


# ============================================================================
# é¢„æµ‹å¯¹æ¯”å¯è§†åŒ–
# ============================================================================

def visualize_prediction_comparison(prediction_results):
    """
    å¯è§†åŒ–ä¸åŒåœºæ™¯ä¸‹çš„é¢„æµ‹ç»“æœå¯¹æ¯”
    
    Parameters:
    -----------
    prediction_results : list
        åŒ…å«å¤šä¸ªé¢„æµ‹ç»“æœçš„åˆ—è¡¨
    """
    print("\nGenerating prediction comparison visualization...")
    
    scenarios = [r['scenario'] for r in prediction_results]
    pred_views = [r['pred_views'] for r in prediction_results]
    pred_lower = [r['pred_views_lower'] for r in prediction_results]
    pred_upper = [r['pred_views_upper'] for r in prediction_results]
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # ç»˜åˆ¶é¢„æµ‹åŒºé—´
    x_pos = np.arange(len(scenarios))
    width = 0.6
    
    # è®¡ç®—åŒºé—´é«˜åº¦
    interval_heights = [u - l for u, l in zip(pred_upper, pred_lower)]
    interval_bottoms = pred_lower
    
    # ç»˜åˆ¶åŒºé—´æ¡å½¢å›¾
    bars = ax.barh(x_pos, interval_heights, left=interval_bottoms, 
                   height=width, alpha=0.3, color='steelblue', 
                   label='95% Prediction Interval')
    
    # ç»˜åˆ¶ç‚¹é¢„æµ‹
    ax.scatter(pred_views, x_pos, color='red', s=200, zorder=5, 
              label='Point Prediction', marker='o', edgecolors='black', linewidths=2)
    
    # æ·»åŠ è¯¯å·®æ£’
    errors_lower = [p - l for p, l in zip(pred_views, pred_lower)]
    errors_upper = [u - p for u, p in zip(pred_upper, pred_views)]
    ax.errorbar(pred_views, x_pos, 
                xerr=[errors_lower, errors_upper],
                fmt='none', ecolor='red', elinewidth=2, capsize=8, capthick=2, zorder=4)
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_yticks(x_pos)
    ax.set_yticklabels([s.replace('_', ' ').title() for s in scenarios], fontsize=12)
    ax.set_xlabel('Predicted Views', fontsize=13, fontweight='bold')
    ax.set_title('Prediction Comparison: Different Scenarios', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    ax.legend(loc='best', fontsize=11)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (pred, lower, upper) in enumerate(zip(pred_views, pred_lower, pred_upper)):
        ax.text(pred, i, f'  {pred:,.0f}', ha='left', va='center', 
               fontsize=11, fontweight='bold')
        ax.text(lower, i, f'{lower:,.0f}  ', ha='right', va='center', 
               fontsize=9, color='blue', alpha=0.7)
        ax.text(upper, i, f'  {upper:,.0f}', ha='left', va='center', 
               fontsize=9, color='blue', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig('prediction_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ“ Prediction comparison plot saved as 'prediction_comparison.png'")
    plt.show()


# ============================================================================
# ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å›å½’åˆ†ææµç¨‹
    """
    print("\n" + "=" * 80)
    print("Hurdle Model - Stage 2: Regression Model Analysis")
    print("=" * 80)
    print("\nThis script will execute the following steps:")
    print("  1. Data loading and preprocessing")
    print("  2. Variable transformation and feature engineering (Log transformation)")
    print("  3. Building OLS model")
    print("  4. Model diagnostics (residual analysis, heteroscedasticity test, robust SE)")
    print("  5. Prediction and interval estimation")
    print("=" * 80)
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = 'New_Youtube_Videos_2022_Transformed.csv'
    
    try:
        # Step 0: åŠ è½½æ•°æ®
        df = load_and_prepare_data(data_file)
        
        # Step 1: ç‰¹å¾å·¥ç¨‹
        df = feature_engineering(df)
        
        # Step 2: å»ºç«‹ OLS æ¨¡å‹
        model, formula, df = build_ols_model(df)
        
        # Step 3: æ¨¡å‹è¯Šæ–­
        robust_model = model_diagnostics(model, df)
        
        # å¦‚æœå­˜åœ¨å¼‚æ–¹å·®ï¼Œä½¿ç”¨ç¨³å¥æ¨¡å‹è¿›è¡Œé¢„æµ‹
        final_model = robust_model if robust_model is not None else model
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹çš„ç³»æ•°ï¼ˆç”¨äºåŸå‹ç³»ç»Ÿï¼‰
        # å¦‚æœä½¿ç”¨äº†ç¨³å¥æ ‡å‡†è¯¯ï¼Œä¿å­˜ç¨³å¥æ¨¡å‹çš„ç³»æ•°
        print("\n" + "=" * 80)
        print("Saving Final Model Coefficients for Prototype System")
        print("=" * 80)
        save_model_coefficients(final_model, formula)
        
        # Step 4: é¢„æµ‹ä¸åŒºé—´ä¼°è®¡
        # ç¤ºä¾‹1ï¼šé«˜æ½œåŠ›åœºæ™¯
        pred_result1 = prediction_with_intervals(final_model, df, scenario='high_potential')
        
        # ç¤ºä¾‹2ï¼šå…¸å‹åœºæ™¯
        print("\n" + "=" * 80)
        pred_result2 = prediction_with_intervals(final_model, df, scenario='typical')
        
        # ç¤ºä¾‹3ï¼šä½æ½œåŠ›åœºæ™¯ï¼ˆå¯¹æ¯”ï¼‰
        print("\n" + "=" * 80)
        pred_result3 = prediction_with_intervals(final_model, df, scenario='low_potential')
        
        # å¯¹æ¯”åˆ†æ
        print("\n" + "=" * 80)
        print("Comparison Analysis: Different Scenarios")
        print("=" * 80)
        print(f"{'Metric':<30} {'High Potential':<20} {'Typical':<20} {'Low Potential':<20}")
        print("-" * 90)
        print(f"{'Predicted Views':<30} {pred_result1['pred_views']:<20,.0f} {pred_result2['pred_views']:<20,.0f} "
              f"{pred_result3['pred_views']:<20,.0f}")
        print(f"{'95% CI Lower':<30} {pred_result1['pred_views_lower']:<20,.0f} {pred_result2['pred_views_lower']:<20,.0f} "
              f"{pred_result3['pred_views_lower']:<20,.0f}")
        print(f"{'95% CI Upper':<30} {pred_result1['pred_views_upper']:<20,.0f} {pred_result2['pred_views_upper']:<20,.0f} "
              f"{pred_result3['pred_views_upper']:<20,.0f}")
        
        # å¯è§†åŒ–é¢„æµ‹å¯¹æ¯”
        visualize_prediction_comparison([pred_result1, pred_result2, pred_result3])
        
        print("\n" + "=" * 80)
        print("Analysis Complete!")
        print("=" * 80)
        print("\nGenerated files:")
        print("\nGenerated files:")
        print("  ğŸ“Š Data Files (for prototype system):")
        print("     - model_coefficients.json: Model coefficients (JSON format)")
        print("     - model_coefficients.csv: Model coefficients (CSV format, human-readable)")
        print("  ğŸ“ˆ Visualization Files:")
        print("     - model_diagnostics.png: Model diagnostic plots (residual analysis, Q-Q plots, etc.)")
        print("     - coefficient_importance.png: Coefficient importance visualization")
        print("     - actual_vs_predicted.png: Actual vs predicted values comparison")
        print("     - prediction_result.png: Prediction result plot (with prediction intervals)")
        print("     - prediction_comparison.png: Comparison of different prediction scenarios")
        
    except FileNotFoundError:
        print(f"\nâŒ Error: Data file '{data_file}' not found")
        print("   Please ensure the data file is in the current directory")
    except Exception as e:
        print(f"\nâŒ Error occurred: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
