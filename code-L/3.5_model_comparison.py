# 3.5_model_comparison.py
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import joblib
import config

def compare_models():
    """
    å¯¹æ¯” OLS, Ridge, Lasso, ElasticNet å››ç§æ¨¡å‹
    è¿”å›æœ€ä½³æ¨¡å‹å’Œå¯¹æ¯”ç»“æœ
    """
    print(">>> [Step 3.5] å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ...")
    
    # 1. è¯»å–æ•°æ®
    try:
        df = pd.read_csv(config.PROCESSED_DATA_PATH)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {config.PROCESSED_DATA_PATH}")
        return None, None
    
    # æ•°æ®æ¸…æ´—
    print("æ­£åœ¨æ¸…æ´—æ•°æ®...")
    df = df.apply(pd.to_numeric, errors='coerce')
    df.fillna(0, inplace=True)
    df.replace([np.inf, -np.inf], 0, inplace=True)
    df = df.astype(float)
    
    X = df.drop(columns=['log_views'])
    y = df['log_views']
    
    # 2. æ ‡å‡†åŒ–ï¼ˆæ­£åˆ™åŒ–æ¨¡å‹éœ€è¦ï¼‰
    print("æ­£åœ¨æ ‡å‡†åŒ–ç‰¹å¾...")
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X),
        columns=X.columns,
        index=X.index
    )
    
    # 3. åˆ‡åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=config.RANDOM_STATE
    )
    
    # 4. è®­ç»ƒå¤šä¸ªæ¨¡å‹
    models = {}
    results = []
    
    # 4.1 OLSæ¨¡å‹
    print("\n[1/4] è®­ç»ƒ OLS æ¨¡å‹...")
    X_train_const = sm.add_constant(X_train, has_constant='add')
    X_test_const = sm.add_constant(X_test, has_constant='add')
    
    try:
        ols_model = sm.OLS(y_train, X_train_const).fit()
        y_pred_ols = ols_model.predict(X_test_const)
        
        models['OLS'] = ols_model
        results.append({
            'Model': 'OLS',
            'RÂ²': r2_score(y_test, y_pred_ols),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_ols)),
            'MAE': mean_absolute_error(y_test, y_pred_ols)
        })
        print("âœ… OLS æ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âŒ OLS æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    # 4.2 Ridgeæ¨¡å‹ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
    print("[2/4] è®­ç»ƒ Ridge æ¨¡å‹ï¼ˆL2æ­£åˆ™åŒ–ï¼‰...")
    try:
        # ç½‘æ ¼æœç´¢æ‰¾æœ€ä¼˜alpha
        ridge_params = {'alpha': [0.1, 1, 10, 100, 1000]}
        ridge_grid = GridSearchCV(
            Ridge(), ridge_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1
        )
        ridge_grid.fit(X_train, y_train)
        ridge_model = ridge_grid.best_estimator_
        y_pred_ridge = ridge_model.predict(X_test)
        
        models['Ridge'] = ridge_model
        results.append({
            'Model': f'Ridge (Î±={ridge_grid.best_params_["alpha"]})',
            'RÂ²': r2_score(y_test, y_pred_ridge),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_ridge)),
            'MAE': mean_absolute_error(y_test, y_pred_ridge)
        })
        print(f"âœ… Ridge æ¨¡å‹è®­ç»ƒå®Œæˆ (æœ€ä¼˜Î±={ridge_grid.best_params_['alpha']})")
    except Exception as e:
        print(f"âŒ Ridge æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    # 4.3 Lassoæ¨¡å‹ï¼ˆL1æ­£åˆ™åŒ–ï¼Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼‰
    print("[3/4] è®­ç»ƒ Lasso æ¨¡å‹ï¼ˆL1æ­£åˆ™åŒ–ï¼‰...")
    try:
        lasso_params = {'alpha': [0.001, 0.01, 0.1, 1, 10]}
        lasso_grid = GridSearchCV(
            Lasso(max_iter=5000), lasso_params, cv=5, 
            scoring='neg_mean_squared_error', n_jobs=-1
        )
        lasso_grid.fit(X_train, y_train)
        lasso_model = lasso_grid.best_estimator_
        y_pred_lasso = lasso_model.predict(X_test)
        
        # ç»Ÿè®¡Lassoå‰”é™¤çš„ç‰¹å¾æ•°
        n_features_kept = np.sum(np.abs(lasso_model.coef_) > 1e-5)
        n_features_removed = len(lasso_model.coef_) - n_features_kept
        
        models['Lasso'] = lasso_model
        results.append({
            'Model': f'Lasso (Î±={lasso_grid.best_params_["alpha"]}, ä¿ç•™{n_features_kept}/{len(X.columns)}ç‰¹å¾)',
            'RÂ²': r2_score(y_test, y_pred_lasso),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_lasso)),
            'MAE': mean_absolute_error(y_test, y_pred_lasso)
        })
        print(f"âœ… Lasso æ¨¡å‹è®­ç»ƒå®Œæˆ (æœ€ä¼˜Î±={lasso_grid.best_params_['alpha']}, ä¿ç•™{n_features_kept}ç‰¹å¾)")
    except Exception as e:
        print(f"âŒ Lasso æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    # 4.4 ElasticNetæ¨¡å‹ï¼ˆL1+L2æ­£åˆ™åŒ–ï¼‰
    print("[4/4] è®­ç»ƒ ElasticNet æ¨¡å‹ï¼ˆL1+L2æ­£åˆ™åŒ–ï¼‰...")
    try:
        enet_params = {
            'alpha': [0.001, 0.01, 0.1, 1],
            'l1_ratio': [0.1, 0.5, 0.7, 0.9]
        }
        enet_grid = GridSearchCV(
            ElasticNet(max_iter=5000), enet_params, cv=5,
            scoring='neg_mean_squared_error', n_jobs=-1
        )
        enet_grid.fit(X_train, y_train)
        enet_model = enet_grid.best_estimator_
        y_pred_enet = enet_model.predict(X_test)
        
        models['ElasticNet'] = enet_model
        results.append({
            'Model': f'ElasticNet (Î±={enet_grid.best_params_["alpha"]}, l1_ratio={enet_grid.best_params_["l1_ratio"]})',
            'RÂ²': r2_score(y_test, y_pred_enet),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_enet)),
            'MAE': mean_absolute_error(y_test, y_pred_enet)
        })
        print(f"âœ… ElasticNet æ¨¡å‹è®­ç»ƒå®Œæˆ (æœ€ä¼˜Î±={enet_grid.best_params_['alpha']}, l1_ratio={enet_grid.best_params_['l1_ratio']})")
    except Exception as e:
        print(f"âŒ ElasticNet æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    # 5. è¾“å‡ºå¯¹æ¯”ç»“æœ
    if not results:
        print("âŒ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥")
        return None, None
    
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('RÂ²', ascending=False)
    
    print("\n" + "="*70)
    print("æ¨¡å‹å¯¹æ¯”ç»“æœï¼ˆæŒ‰RÂ²æ’åºï¼‰")
    print("="*70)
    print(results_df.to_string(index=False))
    print("="*70)
    
    # 6. ä¿å­˜æœ€ä½³æ¨¡å‹
    best_model_name = results_df.iloc[0]['Model'].split(' ')[0]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
    
    if best_model_name == 'OLS':
        joblib.dump(models['OLS'], config.MODEL_PATH)
        print(f"âœ… OLSæ¨¡å‹å·²ä¿å­˜è‡³: {config.MODEL_PATH}")
    else:
        # ä¿å­˜sklearnæ¨¡å‹å’Œscaler
        best_model_path = config.MODEL_PATH.replace('.pkl', f'_{best_model_name.lower()}.pkl')
        joblib.dump(models[best_model_name], best_model_path)
        scaler_path = best_model_path.replace('.pkl', '_scaler.pkl')
        joblib.dump(scaler, scaler_path)
        print(f"âœ… {best_model_name}æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {scaler_path}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    results_path = config.MODEL_PATH.replace('.pkl', '_comparison.csv')
    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"âœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜è‡³: {results_path}")
    
    return models, results_df

if __name__ == "__main__":
    compare_models()

